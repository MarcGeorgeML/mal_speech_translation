{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6d2e05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is NOT available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU in use:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is NOT available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d1cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing google flan-t5-small for translation refinement\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# c2t model loading example\n",
    "import whisper_s2t\n",
    "from whisper_s2t.backends.ctranslate2.model import BEST_ASR_CONFIG\n",
    "\n",
    "#preprocessing audio\n",
    "import importlib\n",
    "import split_and_preprocess\n",
    "importlib.reload(split_and_preprocess)\n",
    "from split_and_preprocess import process_chunks\n",
    "import merge_audio\n",
    "importlib.reload(merge_audio)\n",
    "from merge_audio import merge_wav_files\n",
    "import os\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "297ef316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb9799d",
   "metadata": {},
   "source": [
    "### Loading the whisper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "354b5df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccd6d2bb6294c7c88df2110e97517e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\mals2t\\lib\\site-packages\\whisper_s2t\\speech_segmenter\\frame_vad.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast()\n",
      "e:\\anaconda3\\envs\\mals2t\\lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ct2_model = whisper_s2t.load_model(\n",
    "    model_identifier=\"large-v2\", \n",
    "    backend='CTranslate2', \n",
    "    compute_type='int8',  # Best for cpu\n",
    "    device=device,\n",
    "    asr_options=BEST_ASR_CONFIG,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb97f38",
   "metadata": {},
   "source": [
    "### Loading google flan-t5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab97599",
   "metadata": {},
   "source": [
    "### Deepseek R1 1.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7e01ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7496fab9794026b4a48f826517c855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\mals2t\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in E:\\.cache\\huggingface\\hub\\models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81ff5b062a54e15b1e7ece349c6a452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e15614e5e2944429f0598782dcd1fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e659cd440d2142579d1a9892cc07f27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24e65fe2e9b4c3d9786eefa77870f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:01<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "model_r1 = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a72cf8",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "718e5921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6244\\1795977638.py:4: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(audio_path, sr=None)\n",
      "e:\\anaconda3\\envs\\mals2t\\lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "c:\\Users\\user\\Desktop\\Programming\\Internships\\The marble jar\\mal_speech_translation\\split_and_preprocess.py:36: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(input_path, sr=None, mono=True)\n",
      "e:\\anaconda3\\envs\\mals2t\\lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved 3 chunks to temp_chunks\n",
      "Processed chunk files: ['temp_chunks\\\\chunk_0_processed.wav', 'temp_chunks\\\\chunk_1_processed.wav', 'temp_chunks\\\\chunk_2_processed.wav']\n"
     ]
    }
   ],
   "source": [
    "audio_path = \"megha.aac\"  # Path to your input audio file\n",
    "temp_dir = \"temp_chunks\"            # Directory to save processed chunks\n",
    "\n",
    "audio, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "chunk_paths, sr = process_chunks(audio_path, temp_dir=temp_dir, chunk_duration=30)\n",
    "print(f\"Processed chunk files: {chunk_paths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5685d154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 3 files into merged_output.wav\n"
     ]
    }
   ],
   "source": [
    "merged_audio_path = \"merged_output.wav\"  # Path to save the merged audio file\n",
    "merge_wav_files(temp_dir, merged_audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5817847",
   "metadata": {},
   "source": [
    "### Translation using whisper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "913b3f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing: 100%|██████████| 100/100 [03:40<00:00,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before IK Kerala, Kerala was divided into four parts. Thiruvidhankur, Kochi, Malabar, South Kannara. The people of these villages were of the same culture, language and way of life. Kerala was formed by combining them. In 1920, the Nagpur Congress decided to form the committee of the organization in a bilingual state. In 1921,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# temp_dir = \"temp_chunks\"\n",
    "lang_codes = ['ml']\n",
    "initial_prompts = [None]\n",
    "# Direct Malayalam-to-English translation\n",
    "tasks = ['translate']\n",
    "files = [\"merged_output.wav\"]  # Use the merged audio file\n",
    "\n",
    "# chunk_paths = sorted(\n",
    "#     [os.path.join(temp_dir, f) for f in os.listdir(temp_dir) if f.endswith('.wav')]\n",
    "# )\n",
    "\n",
    "results = ct2_model.transcribe_with_vad(\n",
    "    files,\n",
    "    lang_codes=lang_codes,\n",
    "    tasks=tasks,\n",
    "    initial_prompts=initial_prompts,\n",
    "    batch_size=32  # or higher if your hardware allows\n",
    ")\n",
    "# all_text = [result[0]['text'] for result in results]\n",
    "# full_transcript = \" \".join(all_text)\n",
    "print(results[0][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb632ae",
   "metadata": {},
   "source": [
    "### Further refining translated text using another llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7fadce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Craft a prompt with instructions and context\n",
    "input_text = (\n",
    "    \"You are an expert translator who can translate malayalam to english. \"\n",
    "    \"Given the following sentence, make it more sensible and fluent in English. \"\n",
    "    \"If the sentence does not make sense, use the words in the sentence to form the most plausible translation that makes sense. \"\n",
    "    \"The translation must be such that if its translated back to malayalam, we get a similar sentence as the original malayalam sentence. \"\n",
    "    \"Here is the sentence:\\n\"\n",
    "    f\"{results[0][0]['text']}\\n\"\n",
    "    \"Improved translation:\"\n",
    ")\n",
    "# Generate the improved translation\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model_r1.generate(**input_ids)\n",
    "improved_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0debbff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"Paraphrase this to correct and fluent English: {full_transcript}\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fa3173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
